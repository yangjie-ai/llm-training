import os
import sys
import logging
import traceback
import datasets
import transformers
import modelscope
import torch
from transformers import TrainerCallback

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s: %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('training.log')
    ]
)


class VerboseTrainingCallback(TrainerCallback):
    """è‡ªå®šä¹‰å›è°ƒå‡½æ•°ï¼Œæä¾›æ›´è¯¦ç»†çš„è®­ç»ƒè¿›åº¦ä¿¡æ¯"""

    def __init__(self):
        self.train_loss = []

    def on_train_begin(self, args, state, control, **kwargs):
        logging.info("ğŸš€ è®­ç»ƒå¼€å§‹")
        self.train_loss = []

    def on_epoch_begin(self, args, state, control, **kwargs):
        logging.info(f"ğŸ“˜ å¼€å§‹ç¬¬ {state.epoch + 1} è½®è®­ç»ƒ")

    def on_step_end(self, args, state, control, **kwargs):
        # æ£€æŸ¥æ˜¯å¦éœ€è¦è®°å½•æ—¥å¿—
        if state.global_step % args.logging_steps == 0:
            # å®‰å…¨è·å–æœ€è¿‘çš„æŸå¤±å€¼
            if state.log_history and 'loss' in state.log_history[-1]:
                current_loss = state.log_history[-1]['loss']
                self.train_loss.append(current_loss)
                logging.info(f"ğŸ”¢ è®­ç»ƒè¿›åº¦: æ­¥æ•° {state.global_step}, æŸå¤± {current_loss:.4f}")

    def on_train_end(self, args, state, control, **kwargs):
        logging.info("âœ… è®­ç»ƒå®Œæˆ")
        logging.info(f"è®­ç»ƒæŸå¤±è®°å½•: {self.train_loss}")


def check_cuda_availability():
    """æ£€æŸ¥CUDAå¯ç”¨æ€§"""
    logging.info("æ£€æŸ¥GPUå¯ç”¨æ€§...")
    logging.info(f"CUDAæ˜¯å¦å¯ç”¨: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        logging.info(f"CUDAè®¾å¤‡æ•°é‡: {torch.cuda.device_count()}")
        logging.info(f"å½“å‰CUDAè®¾å¤‡: {torch.cuda.current_device()}")
        logging.info(f"CUDAè®¾å¤‡åç§°: {torch.cuda.get_device_name(0)}")


def main():
    try:
        # æ£€æŸ¥GPU
        check_cuda_availability()

        # åˆ›å»ºå¿…è¦ç›®å½•
        os.makedirs("./data", exist_ok=True)
        os.makedirs("./LLM05", exist_ok=True)
        os.makedirs("./logs", exist_ok=True)

        # åŠ è½½æ•°æ®é›†
        logging.info("1. åŠ è½½æ•°æ®é›†ä¸­...")
        try:
            raw_datasets = datasets.load_dataset(
                "json",
                data_files="./data/wikipedia-cn-20230720-filtered.json"
            )
        except Exception as e:
            logging.error(f"æ•°æ®é›†åŠ è½½å¤±è´¥: {e}")
            raise

        raw_datasets = raw_datasets["train"].train_test_split(test_size=0.1, seed=2222)
        logging.info(f"è®­ç»ƒé›†å¤§å°: {len(raw_datasets['train'])}")
        logging.info(f"éªŒè¯é›†å¤§å°: {len(raw_datasets['test'])}")

        # ä¸‹è½½å¹¶ä¿å­˜tokenizerå’Œé…ç½®
        logging.info("2. ä¸‹è½½å¹¶ä¿å­˜æ¨¡å‹é…ç½®ä¸åˆ†è¯å™¨...")
        modelscope.AutoConfig.from_pretrained("Qwen/Qwen2.5-0.5B").save_pretrained(
            "Qwen2.5-0.5B"
        )
        modelscope.AutoTokenizer.from_pretrained("Qwen/Qwen2.5-0.5B").save_pretrained(
            "Qwen2.5-0.5B"
        )

        context_length = 512
        tokenizer = transformers.AutoTokenizer.from_pretrained("./Qwen2.5-0.5B")

        # æ•°æ®é¢„å¤„ç†
        def tokenize(element):
            outputs = tokenizer(
                element["completion"],
                truncation=True,
                max_length=context_length,
                return_overflowing_tokens=True,
                return_length=True,
            )
            input_batch = [
                input_ids for length, input_ids in zip(outputs["length"], outputs["input_ids"])
                if length == context_length
            ]
            return {"input_ids": input_batch}

        logging.info("3. åˆ†è¯æ•°æ®é›†...")
        tokenized_datasets = raw_datasets.map(
            tokenize,
            batched=True,
            remove_columns=raw_datasets["train"].column_names
        )

        tokenizer.pad_token = tokenizer.eos_token
        data_collator = transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)

        # æ¨¡å‹é…ç½®
        logging.info("4. å‡†å¤‡æ¨¡å‹é…ç½®...")
        config = transformers.AutoConfig.from_pretrained(
            "./Qwen2.5-0.5B",
            vocab_size=len(tokenizer),
            hidden_size=512,
            intermediate_size=2048,
            num_attention_heads=8,
            num_hidden_layers=12,
            n_ctx=context_length,
            bos_token_id=tokenizer.bos_token_id,
            eos_token_id=tokenizer.eos_token_id,
        )
        model = transformers.Qwen2ForCausalLM(config)
        model_size = sum(t.numel() for t in model.parameters())

        logging.info(f"æ¨¡å‹å¤§å°: {model_size / 1000 ** 2:.1f}M å‚æ•°")

        # è®­ç»ƒå‚æ•°
        logging.info("5. é…ç½®è®­ç»ƒå‚æ•°...")
        args = transformers.TrainingArguments(
            output_dir="LLM05",
            per_device_train_batch_size=16,
            per_device_eval_batch_size=16,
            eval_strategy="steps",
            eval_steps=500,
            logging_steps=50,
            gradient_accumulation_steps=8,
            num_train_epochs=5,
            weight_decay=0.1,
            warmup_steps=200,
            optim="adamw_torch",
            lr_scheduler_type="cosine",
            learning_rate=2e-5,
            save_steps=500,
            save_total_limit=10,
            bf16=True if torch.cuda.is_available() else False,
            logging_dir='./logs'
        )

        # åˆå§‹åŒ–è®­ç»ƒå™¨
        logging.info("6. åˆå§‹åŒ–è®­ç»ƒå™¨...")
        verbose_callback = VerboseTrainingCallback()
        trainer = transformers.Trainer(
            model=model,
            tokenizer=tokenizer,
            args=args,
            data_collator=data_collator,
            train_dataset=tokenized_datasets["train"],
            eval_dataset=tokenized_datasets["test"],
            callbacks=[verbose_callback]
        )

        # å¼€å§‹è®­ç»ƒ
        logging.info("ğŸš€ å¼€å§‹æ¨¡å‹è®­ç»ƒ...")
        trainer.train()

        # ä¿å­˜æ¨¡å‹
        logging.info("7. ä¿å­˜æ¨¡å‹...")
        model.save_pretrained("./LLM05/Weight")
        tokenizer.save_pretrained("./LLM05/Weight")

    except Exception as e:
        logging.error(f"è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()